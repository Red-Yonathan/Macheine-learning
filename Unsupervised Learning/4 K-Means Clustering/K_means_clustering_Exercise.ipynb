{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/Python-Notebook-Banners/Exercise.png\"  style=\"display: block; margin-left: auto; margin-right: auto;\";/>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k52b0R_HjaKt"
   },
   "source": [
    "# Exercise: K-means clustering\n",
    "© ExploreAI Academy\n",
    "\n",
    "In this exercise, we will test our understanding of the core concepts of building optimal K-means clustering models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "By the end of this train, you should be able to:\n",
    "- Implement a K-means clustering model in sklearn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from sklearn import preprocessing\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Data/unsupervised_sprint/Live.csv')\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sm6oc90_x5y-"
   },
   "source": [
    "## The data\n",
    "The dataset we will use in this exercise pertains to Facebook live sellers. It comprises various attributes related to live posts made by sellers on the platform. Each row in the dataset represents a specific live post made by a seller. The data includes attributes such as the type of status or the type of post (e.g., video, photo), the number of reactions, comments, shares, likes, loves, wows, hahas, sad, and angry reactions received by the post.\n",
    "\n",
    "The data was collected to understand the engagement and interaction patterns of Facebook live sellers. By analysing the reactions, comments, and shares received by different types of posts, the aim is to gain insights into the effectiveness of various content formats used by sellers. Understanding these patterns can help in devising strategies to improve engagement, reach, and, ultimately, sales for live sellers on the platform."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aVk7tJ9YjaK1"
   },
   "source": [
    "\n",
    "## Exercises\n",
    "In this exercise, we want to apply K-means clustering to our data in order to segment the live posts made by sellers into distinct groups based on their engagement metrics. By clustering similar posts together, we aim to identify patterns or clusters that can help us understand the characteristics of highly engaging posts versus less engaging ones. This segmentation can provide valuable insights for sellers and marketers to tailor their content strategies more effectively, potentially increasing audience engagement and sales."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Data pre-processing\n",
    "Before we can apply any analysis to our data, we need to ensure that the dataset is clean, structured, and suitable for analysis.\n",
    "\n",
    "#### Exercise 1.1\n",
    "Our dataset contains redundant columns that need to be removed. Identify these columns and drop them from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.2\n",
    "An essential step in our process is identifying potential label columns, which represent the target variables we could cluster on. In the context of unsupervised learning, we don't have a designated target variable to predict; however, we can still identify a column that could contain categorical information for which we could cluster our data on.\n",
    "\n",
    "Examine the dataset's attributes and determine which one best represents the categorical information we can cluster on.\n",
    "\n",
    "> **Hint**: Our dataset has three categorical attributes. That's a good place to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means clustering operates on numerical data and calculates distances between data points based on their numerical attributes. Categorical variables, such as `status_type`, `status_published` and `status_id` cannot be directly used in distance-based algorithms like K-means.\n",
    "\n",
    "We need the status_type column to use as a categorical column to cluster our data on. However, the rest of the categorical columns will not contribute meaningfully to the clustering process. They contain unique identifiers that do not represent meaningful clusters; keeping them in the analysis would add noise and complexity without enhancing the clustering outcome.\n",
    "\n",
    "Furthermore, the `status_type` column needs to be converted into numerical equivalents to make it compatible with the clustering algorithm.\n",
    "\n",
    "Drop the remaining categorical columns and convert the `status_type` column values to their numerical equivalents.\n",
    "\n",
    "> **Hint**: To make this process easier, you can use a library called LabelEncoder, which is a utility that is used in machine learning to transform categorical labels into numerical values. Read about it [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "#Instantiate the encoder\n",
    "le = LabelEncoder()\n",
    "#Your code here\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Feature scaling\n",
    "\n",
    "The next step in our process is feature scaling. By scaling the features to a uniform range, we prevent attributes with larger magnitudes from dominating the distance calculations, thus ensuring more balanced clustering results.\n",
    "\n",
    "Perform feature scaling using `MinMaxScaler` on the dataset to ensure that all features contribute equally to the clustering process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: K-means clustering on a random number of K\n",
    "\n",
    "Now that we have pre-processed and scaled our data, we can now apply K-means clustering to segment the live posts made by sellers into distinct groups based on their engagement metrics. \n",
    "\n",
    "To start with, let's apply the K-means clustering algorithm to the pre-processed and scaled dataset using a random number of clusters (K). The purpose is to initially explore the data's clustering tendencies and assess the model's performance with different K values. By fitting the K-means model with varying numbers of clusters, we can observe how the data partitions into different groups and evaluate the model's clustering quality.\n",
    "\n",
    "#### Exercise 3.1\n",
    "\n",
    "Apply the K-means model using 6 clusters to start and print the coordinates of the centroids of the clusters that were found by the algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.2\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the performance of our model by calculating its silhouette_score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Finding the best K Value\n",
    "\n",
    "An average silhouette score of 0.7561 suggests a relatively good clustering structure, though not as strong as a score closer to 1. \n",
    "\n",
    "Instead of trying to guess the best number of clusters to use, let's use one of the many methods we have at our disposal to find the best K value.\n",
    "\n",
    "#### Exercise 4.1\n",
    "\n",
    "Use the Elbow method to find the best K value for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4.2\n",
    "We see that there is a kink at K=2. Therefore, K=2 can be considered a good number on which to cluster our data.\n",
    "\n",
    "Based on these results, let's evaluate the performance of our model by calculating the `silhouette_score`. However, this time, let's calculate and plot the silhouette scores for different numbers of clusters, with each point on the plot representing the average silhouette score for a specific number of clusters. This will allow us to analyse the plot and determine the optimal number of clusters for our data based on the highest silhouette score.\n",
    "\n",
    "Calculate and plot the silhouette scores for cluster values from `K=2` to `K=10` clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions\n",
    "### Exercise 1: Data pre-processing\n",
    "#### Exercise 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset with redundant columns......\")\n",
    "df.info()\n",
    "df.drop(['Column1', 'Column2', 'Column3', 'Column4'], axis=1, inplace=True)\n",
    "print(\"Dataset without reduntant columns.....\")\n",
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view how many different types of categories there are in the status_id column, if any\n",
    "print(\"There are\",len(df['status_id'].unique()),\"unique labels in the status_id column\")\n",
    "# view how many different types of categories there are in the status_published column, if any\n",
    "print(\"There are\",len(df['status_published'].unique()),\"unique labels in the status_published column\")\n",
    "# view how many different types of categories there are in the status_type column, if any\n",
    "print(\"There are\",len(df['status_type'].unique()), \"unique labels in the status_id column\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total number of rows in our dataset is 7050, which makes `status_id` and `status_published` approximate unique identifiers for each of our instances. Therefore, they are not categorical columns we can use to cluster on.\n",
    "\n",
    "There are four categories of labels in the `status_type` column. This is the categorical column we can use in our analysis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the remaining categorical columns\n",
    "df.drop(['status_id', 'status_published'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "#Instantiate the encoder\n",
    "le = LabelEncoder()\n",
    "df['status_type'] = le.fit_transform(df['status_type'])\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "ms = MinMaxScaler()\n",
    "\n",
    "X_scaled = ms.fit_transform(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3: K-means clustering on a random  number of K\n",
    "#### Exercise 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=6, random_state=0) \n",
    "\n",
    "kmeans.fit(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kmeans.cluster_centers_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Calculate the silhouette score\n",
    "silhouette_avg = silhouette_score(X_scaled, kmeans.labels_)\n",
    "\n",
    "print(\"The average silhouette_score is :\", silhouette_avg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Finding the best K value\n",
    "#### Exercise 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\n",
    "    kmeans.fit(X_scaled)\n",
    "    cs.append(kmeans.inertia_)\n",
    "plt.plot(range(1, 11), cs)\n",
    "plt.title('The Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('CS')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# List to store silhouette scores\n",
    "silhouette_scores = []\n",
    "\n",
    "# Range of clusters to try\n",
    "num_clusters_range = range(2, 11)\n",
    "\n",
    "# Iterate over different numbers of clusters\n",
    "for num_clusters in num_clusters_range:\n",
    "    # Train KMeans model with num_clusters clusters\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n",
    "    kmeans.fit(X_scaled)\n",
    "    \n",
    "    # Calculate silhouette score\n",
    "    silhouette_avg = silhouette_score(X_scaled, kmeans.labels_)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "# Plotting silhouette scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(num_clusters_range, silhouette_scores, marker='o', linestyle='-')\n",
    "plt.title('Silhouette Score for Different Numbers of Clusters')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.xticks(np.arange(min(num_clusters_range), max(num_clusters_range)+1, 1.0))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Differences in Python and library versions may result in varied clustering outcomes, complicating the task of determining the optimal number of clusters. It's crucial to remain mindful of this potential variability during your analysis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot suggests that `5` would be the best K value for our data giving us the highest silhouette score. \n",
    "\n",
    "The results of the elbow method and silhouette scores seem to contradict each other when it comes to the best K value that will give the highest cluster quality. Remember that the elbow method and silhouette scores measure different aspects of cluster quality. While the elbow method provides an indication of the optimal number of clusters based on inertia, silhouette scores assess the compactness and separation of clusters.\n",
    "\n",
    "A possible reason for this contradiction is that the elbow method relies on minimising the **within-cluster sum of squares (inertia)**, which may not always reflect the underlying structure of the data, especially if the clusters are non-spherical or have varying densities. Silhouette scores, on the other hand, consider both cluster cohesion and separation, providing a more nuanced evaluation of cluster quality.\n",
    "\n",
    "In this case, the next step would be to consider different clustering algorithms and quality evaluation methods, which may produce varying results for the same dataset. The elbow method and silhouette scores may reflect the strengths and weaknesses of different algorithms differently.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    "\n",
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/ExploreAI_logos/EAI_Blue_Dark.png\"  style=\"width:200px\";/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "12_kmeans_clustering.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
